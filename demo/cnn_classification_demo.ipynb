{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPs96hvl1uch"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import gen_audio_ops as audio_ops\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EBgMvYTZ3mb"
   },
   "source": [
    "___\n",
    "## EMG Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMqoo4LT5nib"
   },
   "outputs": [],
   "source": [
    "fs = 250\n",
    "\n",
    "task_label_map = {'Angry': 0, 'Chewing': 1, 'Eye': 2, 'Eye-Iso': 3, 'In-Iso': 4, \\\n",
    "                  'Jaw': 5, 'L Gaze-L': 6, 'L Gaze-R': 7, 'Out-Iso': 8, 'Sad': 9, \\\n",
    "                  'Smile-Iso': 10, 'Surprise': 11, 'Swallowing': 12, 'Talk': 13, \\\n",
    "                  'Up Gaze': 14, 'Wrinkle-Iso': 15}\n",
    "\n",
    "rev_map = {value: key for key, value in task_label_map.items()}\n",
    "\n",
    "subject_ids = ['subject0', 'subject1', 'subject2', 'subject3', 'subject4', \\\n",
    "               'subject5', 'subject6', 'subject7', 'subject8', 'subject9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "OrBrH9cR5qsE",
    "outputId": "9ac6bf9a-29ac-43d5-8425-e9af16d9d5c2"
   },
   "outputs": [],
   "source": [
    "# Iterate over each subject's EMG data from their Morning and Evening sessions.\n",
    "# Save the signal segments and associated labels dictionaries that map the subject's\n",
    "# ID to their activities.\n",
    "subject_activities, subject_labels = {sid: [] for sid in subject_ids}, {sid: [] for sid in subject_ids}\n",
    "\n",
    "data_dir = '../data/signal_data'\n",
    "timestamp_dir = '../data/timestamps/'\n",
    "for sid in tqdm(subject_ids):    \n",
    "    for time in ['Morning', 'Evening']:\n",
    "        df = pd.read_csv(os.path.join(data_dir, sid, '{}_{}_separated_signals.csv'.format(sid, time)))\n",
    "        signal_data = df[['EMG Channel 1', 'EMG Channel 2']].to_numpy().T\n",
    "        \n",
    "        event_timestamp_df = pd.read_csv(os.path.join(timestamp_dir, sid, '{}_{}_timestamps.csv'.format(sid, time)))\n",
    "        event_timestamp_info = event_timestamp_df[['Event Start (s)', 'Event Stop (s)', 'Task Label']].to_numpy()\n",
    "        \n",
    "        activities, labels = [], []\n",
    "        for event_start, event_stop, event_label in event_timestamp_info:\n",
    "            activities.append(signal_data[:,int(event_start*fs):int(event_stop*fs)])\n",
    "            labels.append(task_label_map[event_label])\n",
    "        \n",
    "        subject_activities[sid].extend(activities)\n",
    "        subject_labels[sid].extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "OrBrH9cR5qsE",
    "outputId": "9ac6bf9a-29ac-43d5-8425-e9af16d9d5c2"
   },
   "outputs": [],
   "source": [
    "# Iterate over each subject's EMG data from their Morning and Evening sessions.\n",
    "# Save the signal segments and associated labels dictionaries that map the subject's\n",
    "# ID to their activities.\n",
    "subject_activities, subject_labels = {sid: [] for sid in subject_ids}, {sid: [] for sid in subject_ids}\n",
    "\n",
    "data_dir = '../data/signal_data'\n",
    "timestamp_dir = '../data/timestamps/'\n",
    "for sid in tqdm(subject_ids):    \n",
    "    for time in ['Morning', 'Evening']:\n",
    "        df = pd.read_csv(os.path.join(data_dir, sid, '{}_{}_separated_signals.csv'.format(sid, time)))\n",
    "        signal_data = df[['EMG Channel 1', 'EMG Channel 2']].to_numpy().T\n",
    "        \n",
    "        event_timestamp_df = pd.read_csv(os.path.join(timestamp_dir, sid, '{}_{}_timestamps.csv'.format(sid, time)))\n",
    "        event_timestamp_info = event_timestamp_df[['Event Start (s)', 'Event Stop (s)', 'Task Label']].to_numpy()\n",
    "        \n",
    "        activities, labels = [], []\n",
    "        for event_start, event_stop, event_label in event_timestamp_info:\n",
    "            activities.append(signal_data[:,int(event_start*fs):int(event_stop*fs)])\n",
    "            labels.append(task_label_map[event_label])\n",
    "        \n",
    "        subject_activities[sid].extend(activities)\n",
    "        subject_labels[sid].extend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQNIlnEkZtji"
   },
   "source": [
    "___\n",
    "## Extract Training and Test Data\n",
    "\n",
    "Training and testing datasets are established using the previously loaded, labeled signal data.  Since Deep Learning models often require large datasets to learn generalizable functions, data augmentation is employed here in effort to maximize the diversity that we see in the training set.  Each time a signal segment is read into the training data set, multiple random croppings of this segment are also added to the training set.  This is not done for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsDTZFun5tnw"
   },
   "outputs": [],
   "source": [
    "def get_random_crop(emg_data, cropped_len=750):\n",
    "    '''\n",
    "    Given a EMG signal segment of arbitrary length, output a\n",
    "    fixed length signal segment representing the original signal.\n",
    "\n",
    "    input:\n",
    "      emg_data (ndarray): 2 channel EMG signal segment\n",
    "      cropped_len (int): length (in samples) of output segment\n",
    "\n",
    "     output:\n",
    "      cropped_seg (ndarray): the fixed length, random crop of the input\n",
    "      signal segment\n",
    "    '''\n",
    "    emg_data = np.array(emg_data)\n",
    "    if emg_data.shape[-1] == cropped_len:\n",
    "        # Input segment is the desired length\n",
    "        cropped_seg = emg_data\n",
    "\n",
    "    elif emg_data.shape[-1] > cropped_len:\n",
    "        # Input segment is longer than desired.  Select\n",
    "        # a random crop of length `cropped_len` from it.\n",
    "        max_idx = emg_data.shape[-1] - cropped_len\n",
    "        idx = np.random.randint(0, max_idx+1)\n",
    "        cropped_seg = emg_data[:,idx:idx+cropped_len]\n",
    "    else:\n",
    "        # Input segment is shorter than desired.  Pad the input\n",
    "        # signal with zeros, randomly centering the input segment.\n",
    "        cropped_seg = np.zeros((2, cropped_len))\n",
    "        i = 0\n",
    "        while i < cropped_len:\n",
    "            if i + emg_data.shape[-1] <= cropped_len:\n",
    "                cropped_seg[:,i:i+emg_data.shape[-1]] = emg_data\n",
    "            else:\n",
    "                cropped_seg[:,i:i+emg_data.shape[-1]] = emg_data[:,:cropped_len-i]\n",
    "            i += emg_data.shape[-1]\n",
    "    return cropped_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4pCYlCs5v3V"
   },
   "outputs": [],
   "source": [
    "# Acquired fixed-length activity segments for each action\n",
    "X_subject, y_subject = {}, {}\n",
    "\n",
    "# Crop data to a 30 second segment\n",
    "crop_len = fs*30\n",
    "\n",
    "for sid in subject_ids:\n",
    "    X, y = [], []\n",
    "    for i, act in enumerate(subject_activities[sid]):\n",
    "        cropped_activity = get_random_crop(act, crop_len)\n",
    "        act_label = subject_labels[sid][i]\n",
    "        X.append(cropped_activity)\n",
    "        y.append(act_label)\n",
    "    X = np.array(X)\n",
    "    X[:,0,:] = (X[:,0,:] - np.mean(X[:,0,:]))/np.std(X[:,0,:])\n",
    "    X[:,1,:] = (X[:,1,:] - np.mean(X[:,1,:]))/np.std(X[:,1,:])\n",
    "    X_subject[sid] = np.array(X)\n",
    "    y_subject[sid] = np.array(y)\n",
    "    \n",
    "    \n",
    "X_full = np.vstack([X_subject[sid] for sid in subject_ids])\n",
    "y_full = np.hstack([y_subject[sid] for sid in subject_ids])\n",
    "\n",
    "inds = np.random.permutation(X_full.shape[0])\n",
    "cutoff = int(len(inds)*0.8)\n",
    "\n",
    "# Define Training Dataset (With Augmentation)\n",
    "X_train, y_train = X_full[:cutoff], y_full[:cutoff]\n",
    "X_train_augmented, y_train_augmented = [], []\n",
    "for i, act in enumerate(X_train):\n",
    "    for _ in range(10): # Extract 10 random croppings\n",
    "        cropped_activity = get_random_crop(act, crop_len)\n",
    "        act_label = y_train[i]\n",
    "        X_train_augmented.append(cropped_activity)\n",
    "        y_train_augmented.append(act_label)\n",
    "X_train, y_train = np.array(X_train_augmented), np.array(y_train_augmented)\n",
    "\n",
    "# Define Testing Dataset (No Augmentation)\n",
    "X_test, y_test = X_full[cutoff:], y_full[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66Jh0yOOxapY"
   },
   "source": [
    "___\n",
    "## Model Instantiation, Fitting, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "UT5_45wfSURP",
    "outputId": "22416c0f-4737-42e6-b834-0f2826248e3b"
   },
   "outputs": [],
   "source": [
    "def extract_feature(data_in, frame_length=256, frame_step=32):\n",
    "    spectrogram = tf.signal.stft(scipy.signal.detrend(data_in.reshape(-1)), frame_length=frame_length, \\\n",
    "                                 frame_step=frame_step, pad_end=False)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.math.log(spectrogram)/np.log(10)\n",
    "    return spectrogram.numpy().squeeze()\n",
    "\n",
    "def extract_spectrogram_feature(data, labels, frequency_range = (10,125)):\n",
    "    train_spectrogram_feature = np.stack([ np.stack([extract_feature(d[:,None]) for d in datum],axis=-1) for datum in tqdm(data)], axis=0)\n",
    "    if frequency_range:\n",
    "        train_spectrogram_feature = train_spectrogram_feature[:,:,frequency_range[0]:frequency_range[1],:]\n",
    "    return (train_spectrogram_feature, labels)\n",
    "\n",
    "def normalize_spectrogram_feature(train_feature, valid_feature):\n",
    "    train_mean = train_feature.mean(axis=(0,1),keepdims=True)\n",
    "    train_std = train_feature.std(axis=(0,1),keepdims=True)\n",
    "    normalized_train_feature = (train_feature - train_mean)/train_std\n",
    "    normalized_valid_feature = (valid_feature - train_mean)/train_std\n",
    "    return normalized_train_feature, normalized_valid_feature, train_mean, train_std\n",
    "\n",
    "X_train, y_train = extract_spectrogram_feature(X_train, y_train)\n",
    "X_test, y_test = extract_spectrogram_feature(X_test, y_test)\n",
    "\n",
    "normalized_train_feature, normalized_test_feature, train_mean, train_std = \\\n",
    "                            normalize_spectrogram_feature(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udOkz7nw7NEk"
   },
   "outputs": [],
   "source": [
    "def get_model(input_shape, lr=0.01, loss='sparse_categorical_crossentropy'):\n",
    "    model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Conv2D(16, (5,5), strides=(2,2),padding='valid',\n",
    "                                                activation='relu',\n",
    "                                                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                                input_shape=input_shape),\n",
    "                tf.keras.layers.Dropout(0.4),\n",
    "                tf.keras.layers.Conv2D(32, (5,5), strides=(2,2),padding='valid', \n",
    "                                                activation='relu',\n",
    "                                                kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "                tf.keras.layers.MaxPool2D(),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dropout(0.4),\n",
    "                tf.keras.layers.Dense(32,\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                      activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.4),\n",
    "                tf.keras.layers.Dense(16,\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                      activation='softmax')\n",
    "            ])\n",
    "\n",
    "    optim = tf.keras.optimizers.Adam(lr)\n",
    "    model.compile(loss=loss, metrics='accuracy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UF18T9iK7Sl0"
   },
   "outputs": [],
   "source": [
    "input_shape = normalized_train_feature.shape[1:]\n",
    "model = get_model(input_shape, lr=0.0003)\n",
    "\n",
    "#callback\n",
    "best_model_path = f'./cnn_model.h5'\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(best_model_path, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "print(f'Training the model ...')\n",
    "history = model.fit(x=normalized_train_feature, y=y_train, batch_size=256, validation_split=0.2, \\\n",
    "                    epochs=100, callbacks=[model_checkpoint_cb], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "kq-TFrOdGnj4",
    "outputId": "d4d4c6ee-e40d-4622-9716-5d8b32855566"
   },
   "outputs": [],
   "source": [
    "saved_model = get_model(input_shape)\n",
    "saved_model.load_weights(best_model_path)\n",
    "y_pred = np.argmax(saved_model.predict(normalized_test_feature), axis=-1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "kq-TFrOdGnj4",
    "outputId": "d4d4c6ee-e40d-4622-9716-5d8b32855566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.19      0.21        16\n",
      "           1       0.94      0.85      0.89        20\n",
      "           2       0.78      0.88      0.82        16\n",
      "           3       1.00      0.18      0.30        17\n",
      "           4       0.50      0.18      0.26        17\n",
      "           5       0.88      0.82      0.85        17\n",
      "           6       0.52      0.61      0.56        18\n",
      "           7       0.40      0.35      0.38        17\n",
      "           8       0.33      0.25      0.29        12\n",
      "           9       0.28      0.55      0.37        20\n",
      "          10       0.59      0.67      0.62        15\n",
      "          11       0.11      0.12      0.12        16\n",
      "          12       0.50      0.22      0.31         9\n",
      "          13       0.57      0.67      0.62        18\n",
      "          14       0.11      0.12      0.12         8\n",
      "          15       0.12      0.20      0.15        15\n",
      "\n",
      "    accuracy                           0.46       251\n",
      "   macro avg       0.49      0.43      0.43       251\n",
      "weighted avg       0.52      0.46      0.45       251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saved_model = get_model(input_shape)\n",
    "saved_model.load_weights(best_model_path)\n",
    "y_pred = np.argmax(saved_model.predict(normalized_test_feature), axis=-1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "spectrogram_cnn_sample.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
